[
["index.html", "Just a Recommender System Based on Google Applications Data Chapter 1 Abstract", " Just a Recommender System Based on Google Applications Data Alfredo Méndez, Eduardo D. Martínez, Alaín Cabrera, Pedro Hernández - ITAM 2017-05-23 Chapter 1 Abstract This study examined the development of a Recommender System based on Google Takeout data. This system is built on a web application that is will be documented on this paper. The main motivation to develop it is this following concept: Make the Google Data accessible to everyone. We already know that every person that has a Google account is able to download their information, but not everyone has the ability to handle or even understand what mbox or json files are. Then we created a web app that allows every person can “read” their own data. On this document, the reader will find out a little view of the process of the development itself. First, is been used the anonymized data from project’s volunteers, it is been downloaded the historical records of Gmail, Location and Searching from Google. Then It was driven an exploratory data analysis in order to understand the nature of features. After that, it is been predicted the most frequent places the individual has been, this step using Non-supervised Machine Learning algorithms based on density. It is been found the network that the individual can be influenced by, and all the important keywords used, in order to filter topics the person is likely to find out using text mining algorithms. All the process it is been documented and it is open code, the reader can easily try the beta version or reproduce the app instead. There is a lot of work left to do, but it is been settle the first steps, one thing is sure: one can find out that sites of interest around the most visited places for a person, can be high accuracy recommended based on searching history, and networking. "],
["introduction.html", "Chapter 2 Introduction 2.1 Go Google Yourself! 2.2 Google Takeout 2.3 Recommender Systems", " Chapter 2 Introduction 2.1 Go Google Yourself! Let’s ask something: What if someone wants to know what the internet says about him/her? This question has been asked several times, and there’s a bunch of actors involved on this topic. But let’s rethink the question to make it a little bit more realistic; What if one want to knows what Google know about him/her? As we might think this is a bit clear to understand, it is less ambiguous and answerable. One of the most important motivations of this project is getting closer to that answer. Everyone spend so much time a day on the web, on different purposes, and there are some companies that they actually have our navigation data, as Google, even though is free and downloadable, one should have certain knowledge to get insights about our data. The goal of the project is getting the Google data insights accessible to everyone. On the other hand, is a little bit more optimistic, to think that everything in the world is going to be solved with Recommender Systems based on one data, that is a lie, but the truth is the world is shaping on data driven things , and as the number of items in markets increases, the quantity of available choices also increases. Consequently, users might face lots of irrelevant data when looking for an item. This problem could be felt when users face numerous items which can confuse them to choose what they are looking for, that is why Recommender System have achieved immerse in consumer experience. Another motivation for the study is reaching the different ways a person might have knowledge based on the data we permitted to Google to use. 2.2 Google Takeout First of all, we have to understand the data we are using, Google Takeout is the backup service provided to you through your Google Apps Account. One has to log into those public services in order to have access to this feature. The reader has to make sure is logged into your Google Apps account and then navigate to Google Takeout interface. Google Takeout was created by the Google Data Liberation Front on June 28, 2011 to allow users to export their data from most of Google’s services. Since its creation, Google has added several more services to Takeout due to popular demand for the user.[1] The user can select to export all the available services or choose services from the above list. Takeout (Takeaway in other languages) will then process the request and put all the files into a zip file. Takeout then optionally sends an email notification that the export is completed, at which point the user can download the archive from the downloads section of the website. The zip file contains a separate folder for each service that was selected for export. One might say, the developers can access other’s data, but one have to be careful because of the personal data rights, in general whether information relates to an identified or identifiable individual, this should be always clear to understand when one is using other’s data. The privacy may be an archaic term when used in reference to depositing information online. Unlike writing a note of secrecy and keeping it safely guarded inside a vault, keeping information hidden and secure online is radically different. We live in an age where we all feel like rulers to our information, kings and queens of online accounts, yet we are not, maybe because of the lack of information on this matter.[2] 2.3 Recommender Systems Recommender Systems are software tools and techniques providing suggestions for items to be of use to a user. The suggestions relate to various decision-making processes, such as what items to consume.[3] It is clear to scope the new notion, see data as a currency, and Recommendation Systems are tools capable of predicting the preferences of users over sets of items (given the historical user-preference data). These systems can be found almost everywhere in the digital space (e.g. Amazon, Google, Netflix), shaping the choices we make, the products we buy, the books we read, or movies we watch. Al those are typically produced a list of recommendations in one of two ways: Through collaborative and content-based filtering or the personality-based approach. Collaborative filtering approaches building a model from a user’s past behavior (items previously purchased or selected and/or numerical ratings given to those items) as well as similar decisions made by other users. This model is then used to predict items (or ratings for items) that the user may have an interest in. Moreover, Recommender Systems always provides the users with the best available choice. In this case, a recommendation model can be applied in order to help users to find what they are looking for faster and easier. In general, the model analyzes the existing data to generate the recommendation list. For this study, It is been going to merge the data from people on the most common Google applications. As a result, users could find what they are looking for, on their favorites places around in a shorter time and desirable a higher precision. It is important to say that optimize the algorithms is the very next step.[4] "],
["the-shape-of-the-data.html", "Chapter 3 The Shape of The Data 3.1 Google Data 3.2 Search 3.3 Locations 3.4 Emails", " Chapter 3 The Shape of The Data 3.1 Google Data On every field of Data Science, the researcher has to be clear about the data sources is using, it is important to identify the variety and the types of format is built of. First, let’s take a look at the data and its nature, so after downloading Google Takeout Data one has to be able to handle the data format. As is said before, this study used three Google sources, Mail, Searches and Locations, because is considered the most used for an average person nowadays, on forwarding studies and developments, is planned to add more Google applications, e.g. Google Chrome history, Contacts, YouTube, and Drive are the most interesting. The file format used for Search and Location history is json there some approaches to use this kind of data, and is getting more popular and well documented, on jq Manual is very clear to see. On the other hand, the transformation of data from mbox to readable is quite more difficult because does not have a regular shape, we defined where each email is identified and separated from each one part of the same body, anyway there are some Python libraries that are very helpful. Part of the Exploratory Data Analysis is to create a Graph DataBase to represent the network of the mails and identify the people with whom the user have more interaction, the tool used is Neo4j. Finally, we extracted keywords of the mails and searches to detect important topics in them. The researcher has to notice that one has to start with some questions, and actually before to beginning to answer ourselves it is been found that there are some general observations when the datasets was explored. The amount of data varies for each account and for the cases explored, the data are available since 2011 (when Google Takeout project starts). Even though all the Search data is splited on various json files of 100 kb each (on average). A user has Location History as long as the user gives permission on the cellphone, actually, it is been found out the location accuracy got better over the time, presumably because the GPS got better over the time as well. Furthermore, we have to settle some questions about how can we get the data. 3.2 Search What insights can we get? How are searches by hour, day, month? Are there long search times? Productive searches? Do the most wanted words say anything? How the data looks like? {&quot;query&quot;: {&quot;id&quot;: [{&quot;timestamp_usec&quot;:&quot;1407774749032392&quot;}], &quot;query_text&quot;:&quot;banco mundial&quot;}} {&quot;query&quot;: {&quot;id&quot;: [{&quot;timestamp_usec&quot;:&quot;1407774749075527&quot;}], “query_text&quot;:&quot;data lake&quot;}} {&quot;query&quot;: {&quot;id&quot;: [{&quot;timestamp_usec&quot;:&quot;1407774749095273&quot;}], “query_text”:&quot;shiba dog&quot;}} 3.3 Locations What insights can we get? What is the frequency of movements? Can work and home be identified? When is a move identified? What are the average transfers in time and distance? How the data looks like? {&quot;timestampMs&quot;: &quot;1414819151315&quot;, &quot;latitudeE7&quot; : 204435729, &quot;longitudeE7&quot; : -872882348, &quot;accuracy&quot; : 49, &quot;activitys&quot; : [ { &quot;timestampMs&quot;: &quot;1414819136573&quot;, &quot;activities&quot; : [ { &quot;type&quot; : &quot;inVehicle&quot;, &quot;confidence&quot; : 62 }, { &quot;type&quot; : &quot;still&quot;, &quot;confidence&quot; : 29 }, { &quot;type&quot; : &quot;onBicycle&quot;, &quot;confidence&quot; : 5 }, { &quot;type&quot; : &quot;unknown&quot;, &quot;confidence&quot; : 5 } ] } ] } 3.4 Emails What insights can we get? How is the traffic over time? Is it possible to make a network of people? What is the relationship of sent with received? Does the subject matter mean anything? How the data looks like? X-GM-THRID: 1545043292255087830 X-Gmail-Labels: Importante,Destacados,Recibidos From: &lt;ventasweb@interjet.com.mx&gt; To: &lt;xxx@gmail.com&gt; Reply-To: &lt;ventasweb@interjet.com.mx&gt; Date: Fri, 9 Sep 2016 19:41:43 -0500 Subject: Interjet Itinerario Content-Type: multipart/alternative; Message-ID: &lt;e34b7917-c506-4a84-ac90-626bf8fafb7a Content-Transfer-Encoding: quoted-printable —CONTENT— 3.4.1 Data Pipeline In order to conduct the study and the application development we have to try to answer those questions and have a scope to get information with data merged to make the recommender system accurate. We also might make a list of reachable tasks: + Estimate a level of area as neighborhood or city where the user has lived or worked, let’s call them, Favorite Places. + Identify, through locations combined with searches, tastes or additional activities of the user. + Find a network related with searches + Estimate what the user is engaged in, by correlating emails and searches. + Make clusters in order to find places of interest + Generate recommendations based on the profile of what you consume daily + Frequencies for each location level for time window + Extract “zone” more frequently and define it as “residence city”. A high leve pipeline could be Figure 1. The Data Pipeline of Go Google Yourself! "],
["exploratory-data-analysis.html", "Chapter 4 Exploratory Data Analysis 4.1 Network Analysis 4.2 E-mails Analysis 4.3 Locations 4.4 Searches", " Chapter 4 Exploratory Data Analysis On every Data Science study the researcher has to follow a certain path, at this stage, once the laborious task of data manipulation is done, the next step to follow in the process is to become intimately familiar with the data set by performing Exploratory Data Analysis (EDA). The way to gain this level of understanding is to utilize the features of the statistical environment the researcher is using that support this effort.[5] It is always a good idea to explore a data set with multiple exploratory techniques, especially when they can be done together for comparison. Naturally, every dataset is different, sometimes one has to deal with many features or many samples, or just the opposite. The notion that has been followed is document oriented because Google data give text, we might say that every search or every mail is a document that belongs into a Corpus. For this development, it ain’t has a large amount of features but and important amount of effort to merge the data. Then, it is quite possible that one may need to revisit one or more data manipulation tasks in order to refine or transform the data even further. The goal of exploratory data analysis is to obtain confidence in the data to a point where one is ready to engage a machine learning algorithms. On this section the reader is going to be able to replicate the Exploratory Data Analysis despiting the web app itself, with the only intention to make clear that before make modeling the researcher has to know the insights the data can say, on this study, the problem is not the number or complexity of features but how connected they are because is all about text and georeference. 4.1 Network Analysis What if because of my networking one is tended to make different choices? Is it the same relevance that one send ten emails to ten persons than send ten emails to two persons? Well, it is quite difficult to see at the first gaze, and maybe we have to make a deeper in-degrees and out-degrees analysis on e-mail networking. But the notion is going to follow is on popular a topic is around the net. On the image is clear to see where is the center of the graph, that center of gravity is the main user related with all the networking, then one can see communities, of course it depends on the user how big the communities is, the sure thing is that hwere is a community is a internet domain related on that community, actually the prediction is going to take the domains along with subjects to find the topics. graph Let’s find the biggest email threads, once we got it, we visualize the longest path, using cypher query language MATCH p=(e:Email)&lt;-[:REPLY*]-(r:Email)&lt;-[]-(sender:Account) WHERE NOT (e)-[:REPLY]-&gt;() RETURN sender.name, e.subject, Id(e), length(p) - 1 AS depth ORDER BY depth DESC To achieve a database based on graph we used a parser built on Go programming language, helped on a ‘.mbox’ reader built in Perl A little summary of email2neo4jpackage: mbox2neo4j is go command line tools that allow you to import your emails from an mbox file into a neo4j graph database. It used a model from the email example project that is explained in Chapter 3 of the book Graph Databases by Ian Robinson, Jim Webber and Emil Eifrem (which is a truly great introduction to the graph btw) First we have to take the attachments of the e-mails On terminal: perl strip-attachments.pl /path/to/mbox/file.mbox Then mbox2neo4j /path/to/mbox/file localhost:7474 4.2 E-mails Analysis It is important to know the distribution of the emailing traffic, then is suitable to know when recommend, as we see before the more we know the user data the accurate the recommendation. The e-mail traffic analysis through the time is partially based on Geoff Boeing work, and it is been developed on python import mailbox, pandas as pd, numpy as np import matplotlib.pyplot as plt, matplotlib.font_manager as fm from dateutil.parser import parse as parse_datetime %matplotlib inline # define the fonts to use for plots #family = &#39;Myriad Pro&#39; family = &#39;serif&#39; title_font = fm.FontProperties(family=family, style=&#39;normal&#39;, size=20, weight=&#39;normal&#39;, stretch=&#39;normal&#39;) label_font = fm.FontProperties(family=family, style=&#39;normal&#39;, size=16, weight=&#39;normal&#39;, stretch=&#39;normal&#39;) ticks_font = fm.FontProperties(family=family, style=&#39;normal&#39;, size=12, weight=&#39;normal&#39;, stretch=&#39;normal&#39;) Load the Gmail archive and parse dates/times from messages # load the mbox file #path = &#39;Destacados.mbox&#39; path = &#39;/Users/pedrohserrano/google-takeout/Mail/Enviados.mbox&#39; mbox = mailbox.mbox(path) print(&#39;There are {:,} messages in the archive.&#39;.format(len(mbox))) There are 1,699 messages in the archive. The Gmail mbox file includes emails and hangouts chats among its “messages”. Hangouts messages don’t have date/time, so we’ll only parse dates and times from the actual emails, and just ignore the hangouts chats. Also, some chats do have a date. To filter them out, verify that if the message has a label that the label does not include “Chat”. # get a list of the dates/times of all the messages in the mbox all_dates = [] all_times = [] for message in mbox: # it&#39;s an email and not a chat if there&#39;s no label, or if there&#39;s a label but it&#39;s not &#39;chat&#39; if not &#39;X-Gmail-Labels&#39; in message or (&#39;X-Gmail-Labels&#39; in message and not &#39;Chat&#39; in message[&#39;X-Gmail-Labels&#39;]): if &#39;Date&#39; in message and message[&#39;Date&#39;] is not None: try: date, time = str(parse_datetime(message[&#39;Date&#39;])).split(&#39; &#39;) except Exception as e: print(e, message[&#39;Date&#39;]) all_dates.append(date) all_times.append(time) else: # hangouts messages have no Date key, so skip them pass print(&#39;There are {:,} messages with dates.&#39;.format(len(all_dates))) There are 1,699 messages with dates. Plot the mail traffic by date # get the count per date date_counts = pd.Series(all_dates).value_counts().sort_index() print(&#39;There are {:,} dates with messages.&#39;.format(len(date_counts))) date_counts.head() # not every date necessarily has a message, so fill in missing dates in the range with zeros date_range = pd.date_range(start=min(all_dates), end=max(all_dates), freq=&#39;D&#39;) index = date_range.map(lambda x: str(x.date())) date_counts = date_counts.reindex(index, fill_value=0) print(&#39;There are {:,} dates total in the range, with or without messages.&#39;.format(len(date_counts))) date_counts.head() # create a series of labels for the plot: each new year&#39;s day xlabels = pd.Series([label if &#39;01-01&#39; in label else None for label in date_counts.index]) xlabels = xlabels[pd.notnull(xlabels)] xlabels.head() # plot the counts per day fig = plt.figure(figsize=[15, 5]) ax = date_counts.plot(kind=&#39;line&#39;, linewidth=0.5, alpha=0.5, color=&#39;g&#39;) ax.grid(True) ax.set_xticks(xlabels.index) ax.set_xticklabels(xlabels, rotation=35, rotation_mode=&#39;anchor&#39;, ha=&#39;right&#39;, fontproperties=ticks_font) ax.set_ylabel(&#39;Number of emails&#39;, fontproperties=label_font) ax.set_title(&#39;Sent mails traffic per day&#39;, fontproperties=title_font) fig.tight_layout() fig.savefig(&#39;images/gmail-traffic-day-destacados.png&#39;, dpi=96) plt.show() gmail-traffic-day-destacados Plot the traffic month by month # get the count per month all_months = [x[:-3] for x in all_dates] month_counts = pd.Series(all_months).value_counts().sort_index() # not every month necessarily has a message, so fill in missing months in the range with zeros date_range = pd.date_range(start=min(all_dates), end=max(all_dates), freq=&#39;D&#39;) months_range = date_range.map(lambda x: str(x.date())[:-3]) index = np.unique(months_range) month_counts = month_counts.reindex(index, fill_value=0) # create a series of labels for the plot: each january xlabels = pd.Series([label if &#39;-01&#39; in label else None for label in month_counts.index]) xlabels = xlabels[pd.notnull(xlabels)] xlabels.head() # plot the counts per month fig = plt.figure(figsize=[15, 5]) ax = month_counts.plot(kind=&#39;line&#39;, linewidth=2.5, alpha=0.6, color=&#39;g&#39;, marker=&#39;+&#39;, markeredgecolor=&#39;g&#39;) ax.grid(True) ax.set_xticks(xlabels.index) ax.set_xticklabels(xlabels, rotation=35, rotation_mode=&#39;anchor&#39;, ha=&#39;right&#39;, fontproperties=ticks_font) ax.set_ylabel(&#39;Number of emails&#39;, fontproperties=label_font) ax.set_title(&#39;Sent mail traffic per month&#39;, fontproperties=title_font) fig.tight_layout() fig.savefig(&#39;images/gmail-traffic-month.png&#39;, dpi=96) plt.show() gmail-traffic-month 4.2.1 Plot the mail traffic by the day of the week # get the count per day of the week day_counts = pd.DataFrame() day_counts[&#39;count&#39;] = date_counts day_counts[&#39;day_of_week&#39;] = date_counts.index.map(lambda x: parse_datetime(x).weekday()) mean_day_counts = day_counts.groupby(&#39;day_of_week&#39;)[&#39;count&#39;].mean() xlabels = [&#39;Monday&#39;, &#39;Tuesday&#39;, &#39;Wednesday&#39;, &#39;Thursday&#39;, &#39;Friday&#39;, &#39;Saturday&#39;, &#39;Sunday&#39;] fig = plt.figure(figsize=[15, 5]) ax = mean_day_counts.plot(kind=&#39;bar&#39;, width=0.6, alpha=0.5, color=&#39;g&#39;, edgecolor=&#39;#333333&#39;) ax.yaxis.grid(True) ax.set_xticklabels(xlabels, rotation=35, rotation_mode=&#39;anchor&#39;, ha=&#39;right&#39;, fontproperties=ticks_font) for label in ax.get_yticklabels(): label.set_fontproperties(ticks_font) ax.set_title(&#39;Sent mails traffic by day of the week&#39;, fontproperties=title_font) ax.set_xlabel(&#39;&#39;) ax.set_ylabel(&#39;Mean number of emails&#39;, fontproperties=label_font) fig.tight_layout() fig.savefig(&#39;images/gmail-traffic-day-week.png&#39;, dpi=96) plt.show() gmail-traffic-day-week 4.2.2 Plot the mail traffic by the hour of the day # get the count per hour of the day times = pd.Series(all_times).map(lambda x: &#39;{:02}:00&#39;.format(parse_datetime(x).hour)) time_counts = times.value_counts().sort_index() time_counts.head() fig = plt.figure(figsize=[15, 5]) ax = time_counts.plot(kind=&#39;bar&#39;, width=0.8, alpha=0.5, color=&#39;g&#39;, edgecolor=&#39;#333333&#39;) ax.yaxis.grid(True) ax.set_xticklabels(time_counts.index, rotation=45, rotation_mode=&#39;anchor&#39;, ha=&#39;right&#39;, fontproperties=ticks_font) for label in ax.get_yticklabels(): label.set_fontproperties(ticks_font) ax.set_title(&#39;Sent mails traffic by hour of the day&#39;, fontproperties=title_font) ax.set_ylabel(&#39;Number of emails&#39;, fontproperties=label_font) fig.tight_layout() fig.savefig(&#39;images/gmail-traffic-hour.png&#39;, dpi=96) plt.show() gmail-traffic-hour 4.2.3 Plot the mail traffic by the minute of the day # get the count per minute of the day, as hh:mm minutes = pd.Series(all_times).map(lambda x: &#39;{:02}:{:02}&#39;.format(parse_datetime(x).hour, parse_datetime(x).minute)) minute_counts = minutes.value_counts().sort_index() # not every minute necessarily has a message, so fill in missing times with zeros time_range = pd.date_range(start=&#39;0:00&#39;, end=&#39;23:59&#39;, freq=&#39;1min&#39;) index = time_range.map(lambda x: &#39;{:02}:{:02}&#39;.format(x.hour, x.minute)) minute_counts = minute_counts.reindex(index, fill_value=0) # create a series of labels for the plot: each new hour xlabels = pd.Series([label if &#39;:00&#39; in label else None for label in minute_counts.index]) xlabels = xlabels[pd.notnull(xlabels)] # plot the counts per minute fig = plt.figure(figsize=[15, 5]) ax = minute_counts.plot(kind=&#39;line&#39;, linewidth=0.7, alpha=0.7, color=&#39;g&#39;) ax.grid(True) ax.set_xticks(xlabels.index) ax.set_xticklabels(xlabels, rotation=45, rotation_mode=&#39;anchor&#39;, ha=&#39;right&#39;, fontproperties=ticks_font) ax.set_ylabel(&#39;Number of emails&#39;, fontproperties=label_font) ax.set_title(&#39;Sent mails traffic by minute of the day&#39;, fontproperties=title_font) fig.tight_layout() fig.savefig(&#39;images/gmail-traffic-minute.png&#39;, dpi=96) plt.show() gmail-traffic-hour As we can see here if we want to develop a recomender system around this user better we send ads based on how likely the user is online, we can know that when he/she is writing emails down. For this user the most likely time is around 10 to eleven and 16 to 17, Tuesdays and Thursdays, and also is important to say that there’s such an active account between November and February. 4.3 Locations The main input we used is the location history file, that is because the application is going to recommend the sites of interest around the most visited places. In other words we seek to describe the daily activity of the person and define their most frequent places (home, office or school). During the day, a person performs two types of activities: staying in one place or moving to another. When a person visits a place, they tend to stay at that location for at least 10 minutes (can be extended to hours in home, office or school cases) which generates at least 17 location records. When the person moves from one place to another, a series of location records are generated along the transited space. These records may or may not be close (geographically) depending on the transfer speed. If you are in a congestion, it is possible, although unlikely, to confuse the place as a visit (depending on the tolerance we take for each visit). As a result of the day, location records generate a cloud of points with areas of high and low density. In order to define the places visited during the day, it is proposed to use the DBSCAN Model which is an unsupervised clusterization algorithm based on density. When we used, the DBSCAN, unclassified points and points clusters are obtained. It will be assumed that points within a cluster are variations due to GPS accuracy or internal movements within the building. Also, within the clusters are the last records obtained during the displacement towards said cluster and the first records of the transfer to the next place. In order to define the visited location (the main one), the average of the records of each cluster will be taken excluding the first and the last in order to reduce the error caused by clustering. The unlabeled points correspond to transfer location records. Also, was quite hard to define the frequently places, so we used a window of 30 days in order to detect if the period of analysis is a common period or a change of address (work or school) or a holiday period. Using the data of the volunteers for this study, we found out that there are approximately 816 registered locations per day, approximately 34 records per hour or one record every 34 seconds. import numpy as np from sklearn.cluster import DBSCAN from sklearn import metrics #from sklearn.datasets.samples_generator import make_blobs from sklearn.preprocessing import StandardScaler import pandas as pd import json import simplejson import datetime import calendar from urllib.request import urlopen,quote import os import webbrowser #import operator #with open(&#39;Historialdeubicaciones.json&#39;, &#39;r&#39;) as fh: with open(&#39;LocationHistory2.json&#39;, &#39;r&#39;) as fh: raw = json.loads(fh.read()) ld = pd.DataFrame(raw[&#39;locations&#39;]) file = open(&quot;dia.csv&quot;,&quot;w&quot;) for i in range(len(ld)): file.write(&quot;{0:.7f}&quot;.format(ld[&#39;latitudeE7&#39;][i]/10000000)+&quot;,&quot;+&quot;{0:.7f}&quot;.format(ld[&#39;longitudeE7&#39;][i]/10000000)+&#39;,&#39;+ld[&#39;timestampMs&#39;][i]+&#39;,&#39;+ datetime.datetime.fromtimestamp( int(ld[&#39;timestampMs&#39;][i])/ 1e3 ).strftime(&#39;%Y-%m-%d&#39;)+&#39;,&#39;+datetime.datetime.fromtimestamp( int(ld[&#39;timestampMs&#39;][i])/ 1e3 ).strftime(&#39;%H:%M:%S&#39;) +&#39;,&#39; +calendar.day_name[datetime.datetime.fromtimestamp(int(ld[&#39;timestampMs&#39;][i])/ 1e3 ).weekday()]+ &#39;\\n&#39;) file.close() coords=pd.read_csv(&#39;dia.csv&#39;, names = [&quot;lat&quot;, &quot;lon&quot;,&quot;timestamp&quot;,&quot;fecha&quot;,&quot;hora&quot;,&quot;dia&quot;]) Define frequent places: 30 day window coords3=coords[coords.fecha==coords[&#39;fecha&#39;].unique()[0]] hours=[&#39;00:00:00&#39;,&#39;01:00:00&#39;, &#39;02:00:00&#39;,&#39;03:00:00&#39;, &#39;04:00:00&#39;,&#39;05:00:00&#39;, &#39;06:00:00&#39;,&#39;07:00:00&#39;, &#39;08:00:00&#39;,&#39;09:00:00&#39;, &#39;10:00:00&#39;,&#39;11:00:00&#39;, &#39;12:00:00&#39;,&#39;13:00:00&#39;, &#39;14:00:00&#39;,&#39;15:00:00&#39;, &#39;16:00:00&#39;,&#39;17:00:00&#39;, &#39;18:00:00&#39;,&#39;19:00:00&#39;, &#39;20:00:00&#39;,&#39;21:00:00&#39;, &#39;22:00:00&#39;,&#39;23:00:00&#39;,&#39;23:59:59&#39;] inicio=0 final=10 for i in range(inicio,final): coords3=coords3.append(coords[coords.fecha==coords[&#39;fecha&#39;].unique()[1+i]]) print(coords[&#39;fecha&#39;].unique()[1+inicio],coords[&#39;fecha&#39;].unique()[1+final]) cosa=coords3[[&#39;lat&#39;,&#39;lon&#39;]] cosa = cosa.reset_index(drop=True) min_samples=np.max([20,len(cosa)*.07]) scaler = StandardScaler() scaler.fit(cosa) X=scaler.fit_transform(cosa) direcciones={} db = DBSCAN(eps=0.031, min_samples=min_samples).fit(X) core_samples_mask = np.zeros_like(db.labels_, dtype=bool) core_samples_mask[db.core_sample_indices_] = True labels = db.labels_ cosa=cosa.iloc[db.core_sample_indices_] cosa = cosa.reset_index(drop=True) recuento={} # Number of clusters in labels, ignoring noise if present. n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0) import matplotlib.pyplot as plt unique_labels = set(labels) colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels))) clusters = [X[labels == i] for i in range(n_clusters_)] markers=&quot;&quot;&quot; &quot;&quot;&quot; places=&quot;AIzaSyCsgMwi_tzAVkae-8Rq9v2A_kjeJF5L2kU&quot; c0=scaler.inverse_transform(clusters[0]) c0r=pd.DataFrame(data=c0[0:,0:]) c0r.columns = [&#39;lat&#39;, &#39;lon&#39;] c0r[&#39;cluster&#39;]=0 casa={} matutino={} for i in range(n_clusters_): c0=scaler.inverse_transform(clusters[i]) c0r=pd.DataFrame(data=c0[0:,0:]) c0r.columns = [&#39;lat&#39;, &#39;lon&#39;] c0r[&#39;cluster&#39;]=i aux=c0r.drop_duplicates() aux=aux.reset_index(drop=True) horas=coords3[(coords3[&#39;lat&#39;]==aux[&#39;lat&#39;].loc[0]) &amp;(coords3[&#39;lon&#39;]==aux[&#39;lon&#39;].loc[0])] casa[i]=0 matutino[i]=0 vespertino=0 diurno=0 for k in range(1,len(aux)): horas=horas.append(coords3[(coords3[&#39;lat&#39;]==aux[&#39;lat&#39;].loc[k]) &amp;(coords3[&#39;lon&#39;]==aux[&#39;lon&#39;].loc[k])]) cosita=datetime.datetime.strptime(np.max(horas[&#39;hora&#39;]),&#39;%H:%M:%S&#39;)-datetime.datetime.strptime(np.min(horas[&#39;hora&#39;]),&#39;%H:%M:%S&#39;) maximo=datetime.datetime.strptime(np.max(horas[&#39;hora&#39;]),&#39;%H:%M:%S&#39;)-cosita*0 minimo=datetime.datetime.strptime(np.min(horas[&#39;hora&#39;]),&#39;%H:%M:%S&#39;)+cosita*0 bajo=&#39;00:00:00&#39; alto=&#39;00:00:00&#39; for alto in hours: temp=horas[(horas[&#39;hora&#39;]&lt;alto)&amp;(horas[&#39;hora&#39;]&gt;bajo)] recuento[alto]=len(temp[&#39;hora&#39;]) if ((alto&lt;&#39;07:00:00&#39;)|(alto&gt;&#39;23:00:00&#39;)): casa[i]=casa[i]+len(temp[&#39;hora&#39;]) if ((alto&lt;&#39;17:00:00&#39;)|(alto&gt;&#39;11:00:00&#39;)): matutino[i]=matutino[i]+len(temp[&#39;hora&#39;]) bajo=alto util=horas[(horas[&#39;hora&#39;]&lt;maximo.strftime(&quot;%H:%M:%S&quot;))&amp;(horas[&#39;hora&#39;]&gt;minimo.strftime(&quot;%H:%M:%S&quot;))] if(len(util)&gt;0): lon= np.mean(util[&#39;lon&#39;]) lat= np.mean(util[&#39;lat&#39;]) url_maps=&quot;https://maps.googleapis.com/maps/api/geocode/json?latlng=&quot;+str(lat)+&quot;,&quot;+str(lon)+&quot;&amp;key=AIzaSyCb0Wakn29V87eBdMd_fAb3DGcxAKtqtxY&quot; with urlopen(url_maps) as response: result= simplejson.load(urlopen(url_maps)) direcciones[i]=result[&#39;results&#39;][0][&#39;formatted_address&#39;] url_places1=&quot;https://maps.googleapis.com/maps/api/place/nearbysearch/json?location=&quot;+str(lat)+&quot;,&quot;+str(lon)+&quot;&amp;rankby=distance&quot;+&quot;&amp;types=None&quot;+&quot;&amp;key=&quot;+places markers=markers+&quot;&quot;&quot;var marker = new google.maps.Marker({ map: map, draggable: true, icon: { path: google.maps.SymbolPath.CIRCLE, scale:5 }, position: {lat: &quot;&quot;&quot;+ str(lat) +&quot;&quot;&quot; , lng: &quot;&quot;&quot;+str(lon)+&quot;&quot;&quot;}, title: &#39;&quot;&quot;&quot;+result[&#39;results&#39;][0][&#39;formatted_address&#39;]+&quot;&quot;&quot;cluster: &quot;&quot;&quot;+str(i)+&quot;&quot;&quot;&#39; });&quot;&quot;&quot; centro=&#39;{lat:&#39;+ str(np.mean(cosa[&#39;lat&#39;])) +&quot;&quot;&quot; , lng: &quot;&quot;&quot;+str(np.mean(cosa[&#39;lon&#39;]))+&#39;}&#39; print(casa) print(matutino) print(direcciones) 2017-03-21 2017-03-11 {0: 0, 1: 914, 2: 268, 3: 0} {0: 1386, 1: 3333, 2: 830, 3: 1040} {0: &#39;Edificio 10, Altavista, Ciudad de México, CDMX, Mexico&#39;, 1: &#39;Cerro San Francisco 305, Campestre Churubusco, 04200 Ciudad de México, CDMX, Mexico&#39;, 2: &#39;Cto. Interior Maestro José Vasconcelos 208, Condesa, 06140 Ciudad de México, CDMX, Mexico&#39;, 3: &#39;Torre C, Av Sta Fe 505, Santa Fe, Contadero, 01219 Ciudad de México, CDMX, Mexico&#39;} #print(&#39;Trabajo: &#39;,direcciones[max(matutino, key=matutino.get)]) aux=[k for k, v in casa.items() if v &gt; 0.4*sum(casa.values())] for i in aux: print(&#39;Casa &#39;,i,&#39;: &#39;,direcciones[i]) aux=[k for k, v in matutino.items() if v &gt; sum(matutino.values())/(n_clusters_+1)] for i in aux: print(&#39;Trabajo/Escuela &#39;,i,&#39;: &#39;,direcciones[i]) Casa 1 : Cerro San Francisco 305, Campestre Churubusco, 04200 Ciudad de México, CDMX, Mexico Trabajo/Escuela 0 : Edificio 10, Altavista, Ciudad de México, CDMX, Mexico Trabajo/Escuela 1 : Cerro San Francisco 305, Campestre Churubusco, 04200 Ciudad de México, CDMX, Mexico direcciones Day #with open(&#39;Historialdeubicaciones.json&#39;, &#39;r&#39;) as fh: with open(&#39;LocationHistory2.json&#39;, &#39;r&#39;) as fh: raw = json.loads(fh.read()) ld = pd.DataFrame(raw[&#39;locations&#39;]) file = open(&quot;dia.csv&quot;,&quot;w&quot;) for i in range(len(ld)): file.write(&quot;{0:.7f}&quot;.format(ld[&#39;latitudeE7&#39;][i]/10000000)+&quot;,&quot;+&quot;{0:.7f}&quot;.format(ld[&#39;longitudeE7&#39;][i]/10000000)+&#39;,&#39;+ld[&#39;timestampMs&#39;][i]+&#39;,&#39;+ datetime.datetime.fromtimestamp( int(ld[&#39;timestampMs&#39;][i])/ 1e3 ).strftime(&#39;%Y-%m-%d&#39;)+&#39;,&#39;+datetime.datetime.fromtimestamp( int(ld[&#39;timestampMs&#39;][i])/ 1e3 ).strftime(&#39;%H:%M:%S&#39;) +&#39;,&#39; +calendar.day_name[datetime.datetime.fromtimestamp(int(ld[&#39;timestampMs&#39;][i])/ 1e3 ).weekday()]+ &#39;\\n&#39;) file.close() coords=pd.read_csv(&#39;dia.csv&#39;, names = [&quot;lat&quot;, &quot;lon&quot;,&quot;timestamp&quot;,&quot;fecha&quot;,&quot;hora&quot;,&quot;dia&quot;]) coords2=coords[coords.fecha==&#39;2017-02-02&#39;] cosa=coords2[[&#39;lat&#39;,&#39;lon&#39;]] cosa = cosa.reset_index(drop=True) print(len(cosa)) min_samples=np.max([20,len(cosa)*.05]) print(&#39;min&#39;,min_samples) scaler = StandardScaler() scaler.fit(cosa) X=scaler.fit_transform(cosa) db = DBSCAN(eps=0.085, min_samples=min_samples).fit(X) core_samples_mask = np.zeros_like(db.labels_, dtype=bool) core_samples_mask[db.core_sample_indices_] = True print(len(db.core_sample_indices_)) labels = db.labels_ cosa=cosa.iloc[db.core_sample_indices_] cosa = cosa.reset_index(drop=True) # Number of clusters in labels, ignoring noise if present. n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0) print(&#39;Estimated number of clusters: %d&#39; % n_clusters_) import matplotlib.pyplot as plt unique_labels = set(labels) colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels))) for k, col in zip(unique_labels, colors): if k == -1: # Black used for noise. col = &#39;k&#39; class_member_mask = (labels == k) xy = X[class_member_mask &amp; core_samples_mask] plt.plot(xy[:, 0], xy[:, 1], &#39;o&#39;, markerfacecolor=col, markeredgecolor=&#39;k&#39;, markersize=7) xy = X[class_member_mask &amp; ~core_samples_mask] plt.plot(xy[:, 0], xy[:, 1], &#39;o&#39;, markerfacecolor=col, markeredgecolor=&#39;k&#39;, markersize=2) plt.title(&#39;Estimated number of clusters: %d&#39; % n_clusters_) plt.axis(&#39;off&#39;) plt.show() clusters = [X[labels == i] for i in range(n_clusters_)] markers=&quot;&quot;&quot; &quot;&quot;&quot; places=&quot;AIzaSyCsgMwi_tzAVkae-8Rq9v2A_kjeJF5L2kU&quot; c0=scaler.inverse_transform(clusters[0]) c0r=pd.DataFrame(data=c0[0:,0:]) c0r.columns = [&#39;lat&#39;, &#39;lon&#39;] c0r[&#39;cluster&#39;]=0 intento=c0r for i in range(n_clusters_): c0=scaler.inverse_transform(clusters[i]) c0r=pd.DataFrame(data=c0[0:,0:]) c0r.columns = [&#39;lat&#39;, &#39;lon&#39;] c0r[&#39;cluster&#39;]=i intento=intento.append(c0r) aux=c0r.drop_duplicates() aux=aux.reset_index(drop=True) horas=coords2[(coords2[&#39;lat&#39;]==aux[&#39;lat&#39;].loc[0]) &amp;(coords2[&#39;lon&#39;]==aux[&#39;lon&#39;].loc[0])] for k in range(1,len(aux)): horas=horas.append(coords2[(coords2[&#39;lat&#39;]==aux[&#39;lat&#39;].loc[k]) &amp;(coords2[&#39;lon&#39;]==aux[&#39;lon&#39;].loc[k])]) cosita=datetime.datetime.strptime(np.max(horas[&#39;hora&#39;]),&#39;%H:%M:%S&#39;)-datetime.datetime.strptime(np.min(horas[&#39;hora&#39;]),&#39;%H:%M:%S&#39;) maximo=datetime.datetime.strptime(np.max(horas[&#39;hora&#39;]),&#39;%H:%M:%S&#39;)-cosita*.1 minimo=datetime.datetime.strptime(np.min(horas[&#39;hora&#39;]),&#39;%H:%M:%S&#39;)+cosita*.1 print (i,maximo,minimo) util=horas[(horas[&#39;hora&#39;]&lt;maximo.strftime(&quot;%H:%M:%S&quot;))&amp;(horas[&#39;hora&#39;]&gt;minimo.strftime(&quot;%H:%M:%S&quot;))] if(len(util)&gt;0): lon= np.mean(util[&#39;lon&#39;]) lat= np.mean(util[&#39;lat&#39;]) url_maps=&quot;https://maps.googleapis.com/maps/api/geocode/json?latlng=&quot;+str(lat)+&quot;,&quot;+str(lon)+&quot;&amp;key=AIzaSyCb0Wakn29V87eBdMd_fAb3DGcxAKtqtxY&quot; with urlopen(url_maps) as response: result= simplejson.load(urlopen(url_maps)) print (result[&#39;results&#39;][0][&#39;formatted_address&#39;]) url_places1=&quot;https://maps.googleapis.com/maps/api/place/nearbysearch/json?location=&quot;+str(lat)+&quot;,&quot;+str(lon)+&quot;&amp;rankby=distance&quot;+&quot;&amp;types=None&quot;+&quot;&amp;key=&quot;+places #with urlopen(url_places1) as response: # result_p1= simplejson.load(urlopen(url_places1)) #print (&quot;están en:&quot;,result_p1[&#39;results&#39;][0][&#39;name&#39;],&#39;---&#39;,result_p1[&#39;results&#39;][0][&#39;types&#39;][0]) markers=markers+&quot;&quot;&quot;var marker = new google.maps.Marker({ map: map, draggable: true, icon: { path: google.maps.SymbolPath.CIRCLE, scale:5 }, position: {lat: &quot;&quot;&quot;+ str(lat) +&quot;&quot;&quot; , lng: &quot;&quot;&quot;+str(lon)+&quot;&quot;&quot;}, title: &#39;&quot;&quot;&quot;+result[&#39;results&#39;][0][&#39;formatted_address&#39;]+&quot;&quot;&quot;cluster: &quot;&quot;&quot;+str(i)+&quot;&quot;&quot;&#39; });&quot;&quot;&quot; centro=&#39;{lat:&#39;+ str(np.mean(cosa[&#39;lat&#39;])) +&quot;&quot;&quot; , lng: &quot;&quot;&quot;+str(np.mean(cosa[&#39;lon&#39;]))+&#39;}&#39; hours=[&#39;00:00:00&#39;,&#39;00:15:00&#39;,&#39;00:30:00&#39;,&#39;00:45:00&#39;,&#39;01:00:00&#39;, &#39;01:15:00&#39;,&#39;01:30:00&#39;,&#39;01:45:00&#39;,&#39;02:00:00&#39;,&#39;02:15:00&#39;, &#39;02:30:00&#39;,&#39;02:45:00&#39;,&#39;03:00:00&#39;,&#39;03:15:00&#39;,&#39;03:30:00&#39;, &#39;03:45:00&#39;,&#39;04:00:00&#39;,&#39;04:15:00&#39;,&#39;04:30:00&#39;,&#39;04:45:00&#39;, &#39;05:00:00&#39;,&#39;05:15:00&#39;,&#39;05:30:00&#39;,&#39;05:45:00&#39;,&#39;06:00:00&#39;, &#39;06:15:00&#39;,&#39;06:30:00&#39;,&#39;06:45:00&#39;,&#39;07:00:00&#39;,&#39;07:15:00&#39;, &#39;07:30:00&#39;,&#39;07:45:00&#39;,&#39;08:00:00&#39;,&#39;08:15:00&#39;,&#39;08:30:00&#39;, &#39;08:45:00&#39;,&#39;09:00:00&#39;,&#39;09:15:00&#39;,&#39;09:30:00&#39;,&#39;09:45:00&#39;, &#39;10:00:00&#39;,&#39;10:15:00&#39;,&#39;10:30:00&#39;,&#39;10:45:00&#39;,&#39;11:00:00&#39;, &#39;11:15:00&#39;,&#39;11:30:00&#39;,&#39;11:45:00&#39;,&#39;12:00:00&#39;,&#39;12:15:00&#39;, &#39;12:30:00&#39;,&#39;12:45:00&#39;,&#39;13:00:00&#39;,&#39;13:15:00&#39;,&#39;13:30:00&#39;, &#39;13:45:00&#39;,&#39;14:00:00&#39;,&#39;14:15:00&#39;,&#39;14:30:00&#39;,&#39;14:45:00&#39;, &#39;15:00:00&#39;,&#39;15:15:00&#39;,&#39;15:30:00&#39;,&#39;15:45:00&#39;,&#39;16:00:00&#39;, &#39;16:15:00&#39;,&#39;16:30:00&#39;,&#39;16:45:00&#39;,&#39;17:00:00&#39;,&#39;17:15:00&#39;,&#39;17:30:00&#39;,&#39;17:45:00&#39;, &#39;18:00:00&#39;,&#39;18:15:00&#39;,&#39;18:30:00&#39;,&#39;18:45:00&#39;,&#39;19:00:00&#39;,&#39;19:15:00&#39;,&#39;19:30:00&#39;,&#39;19:45:00&#39;, &#39;20:00:00&#39;,&#39;20:15:00&#39;,&#39;20:30:00&#39;,&#39;20:45:00&#39;,&#39;21:00:00&#39;,&#39;21:15:00&#39;,&#39;21:30:00&#39;,&#39;21:45:00&#39;, &#39;22:00:00&#39;,&#39;22:15:00&#39;,&#39;22:30:00&#39;,&#39;22:45:00&#39;,&#39;23:00:00&#39;,&#39;23:15:00&#39;,&#39;23:30:00&#39;,&#39;23:45:00&#39;] result = pd.merge(coords2, intento,how=&#39;inner&#39;, on=[&#39;lat&#39;, &#39;lon&#39;]) join=result.drop_duplicates() bajo=&#39;00:00:00&#39; alto=&#39;00:00:00&#39; transporte=0 cluster=-20 ultima=np.min(join[&#39;hora&#39;]) for j in range(1,len(hours)): alto=hours[j] chin=join[(join[&#39;hora&#39;]&gt;bajo)&amp;(join[&#39;hora&#39;]&lt;alto)] if len(chin[&#39;cluster&#39;].unique())&gt;1: #print (&#39;Cambio de cluster!!&#39;) conflicto={} for i in chin[&#39;cluster&#39;].unique(): conflicto[i]=np.min(chin[chin[&#39;cluster&#39;]==i][&#39;hora&#39;]) sorted_x = sorted(conflicto.items(), key=operator.itemgetter(1)) print(sorted_x) for ii in sorted_x: i=ii[0] if cluster==i: ultima=np.max(chin[chin[&#39;cluster&#39;]==cluster][&#39;hora&#39;]) else: print(&#39;te fuiste de &#39;,cluster,&#39; a las &#39;,ultima) auxT=ultima cluster=i print(&#39;llegaste a &#39;,cluster, &#39;a las &#39;,np.min(chin[chin[&#39;cluster&#39;]==i][&#39;hora&#39;])) ultima=np.max(chin[chin[&#39;cluster&#39;]==cluster][&#39;hora&#39;]) print(&#39;--Tiempo de traslado: &#39;,(datetime.datetime.strptime(np.min(chin[chin[&#39;cluster&#39;]==i][&#39;hora&#39;]),&#39;%H:%M:%S&#39;)-datetime.datetime.strptime(auxT,&#39;%H:%M:%S&#39;)).seconds) transporte= transporte+(datetime.datetime.strptime(np.min(chin[chin[&#39;cluster&#39;]==i][&#39;hora&#39;]),&#39;%H:%M:%S&#39;)-datetime.datetime.strptime(auxT,&#39;%H:%M:%S&#39;)).seconds else: if len(chin[&#39;cluster&#39;].unique())==1: if cluster==chin[&#39;cluster&#39;].unique()[0]: ultima=np.max(chin[chin[&#39;cluster&#39;]==cluster][&#39;hora&#39;]) else: if cluster==-20: print(&#39;amaneciste en&#39;, chin[&#39;cluster&#39;].unique()[0]) if len(chin[chin[&#39;cluster&#39;]==cluster][&#39;hora&#39;])&gt;0: ultima=np.max(chin[chin[&#39;cluster&#39;]==cluster][&#39;hora&#39;]) auxT=ultima print(ultima,auxT) else: #print(&#39;cambio de cluster &#39;,cluster,&#39; a &#39;,chin[&#39;cluster&#39;].unique()[0]) print(&#39;Te fuiste de &#39;,cluster,&#39; a las &#39;,ultima) auxT=ultima #print(&#39;Ultima ub. registrada: &#39;,np.max(chin[chin[&#39;cluster&#39;]==cluster][&#39;hora&#39;])) # print(&#39;Ultima ub. registrada para &#39;,cluster,&#39;: &#39;,cluster,ultima) #print(chin[chin[&#39;cluster&#39;]==cluster]) cluster=chin[&#39;cluster&#39;].unique()[0] auxT=ultima ultima=np.max(chin[chin[&#39;cluster&#39;]==cluster][&#39;hora&#39;]) print(&#39;Llegaste a &#39;,cluster,&#39; a las: &#39;,np.min(chin[chin[&#39;cluster&#39;]==cluster][&#39;hora&#39;])) print(&#39;--Tiempo de traslado: &#39;,str(datetime.timedelta(seconds=(datetime.datetime.strptime(np.min(chin[chin[&#39;cluster&#39;]==cluster][&#39;hora&#39;]),&#39;%H:%M:%S&#39;)-datetime.datetime.strptime(auxT,&#39;%H:%M:%S&#39;)).seconds))) transporte= transporte+(datetime.datetime.strptime(np.min(chin[chin[&#39;cluster&#39;]==cluster][&#39;hora&#39;]),&#39;%H:%M:%S&#39;)-datetime.datetime.strptime(auxT,&#39;%H:%M:%S&#39;)).seconds #else: # print(&#39;no hay ubicaciones registradas entre &#39;,bajo,&#39; y &#39;,alto) bajo=alto print (&#39;En el día usaste&#39;,str(datetime.timedelta(seconds=transporte)),&#39; para desplazarte&#39;) Output: 0 1900-01-01 22:52:04.600000 1900-01-01 20:23:29.400000 Paseo de la Reforma 50, Miguel Hidalgo, 11550 Ciudad de México, CDMX, Mexico 1 1900-01-01 22:54:49.300000 1900-01-01 18:35:55.700000 Felipe Villanueva 19, Guadalupe Inn, 01020 Ciudad de México, CDMX, Mexico 2 1900-01-01 16:45:28.700000 1900-01-01 10:27:10.300000 Torre C, Av Sta Fe 505, Santa Fe, Contadero, 01219 Ciudad de México, CDMX, Mexico 3 1900-01-01 07:25:18.700000 1900-01-01 00:50:28.300000 Cerro San Francisco 309, Campestre Churubusco, 04200 Ciudad de México, CDMX, Mexico amaneciste en 3 Llegaste a 3 a las: 00:01:07 --Tiempo de traslado: 0:00:00 Te fuiste de 3 a las 08:14:40 Llegaste a 2 a las: 09:39:53 --Tiempo de traslado: 1:25:13 Te fuiste de 2 a las 17:32:46 Llegaste a 1 a las: 18:03:34 --Tiempo de traslado: 0:30:48 Te fuiste de 1 a las 19:48:47 Llegaste a 0 a las: 20:04:55 --Tiempo de traslado: 0:16:08 Te fuiste de 0 a las 23:10:39 Llegaste a 1 a las: 23:21:48 --Tiempo de traslado: 0:11:09 En el día usaste 2:23:18 para desplazarte clusters 4.4 Searches It is very important to get insights of the searches’ data because is going to help to find highlights keywords whose will deprecate the sites of interest the application is going to recommend. When we analyzed the user searches, we considered two approaches that allowed us to detect relevant characteristics and activities of the user, and to use this information obtained in the recommendation. First, to detect topics in a general is to build a full corpus, where we can detect general characteristics of the user using automatic clustering algorithms, then compare the different algorithms outputs and determine the number of topics. Secondly, we seek to analyze the user search periods, once the most important issues are detected in the first step, to detect the periodicity in terms of frequency and search topics. In this way, we will be able to detect if the identified issues correspond to activities that the user still performs, that already has time without realizing them or that has been doing them for a long time, later we use this information in the algorithm of recommendation. In both cases an automatic detection is sought. To achieve these goals we consider the following characteristics of Google searches: They do not require grammatical rules to be able to show a result It is advisable to perform searches without punctuation marks to achieve better results Google is able to interpret languages without specifying the language in which you are writing Considering these characteristics, we observe that the steps for pre-processing the text decrease, and it is also necessary to perform it in different languages, in our case it will be done in English and Spanish, which are the most frequently used languages among our users. Although later research may include automatic language detection tools to perform automatic cleaning of the text, according to the corpus being processed. #!/usr/local/Cellar/python3/3.5.1/bin/python3 import sys import matplotlib.pyplot as plt import numpy as np import pandas as pd if __name__ == &quot;__main__&quot;: todos=[[] for i in range(7)] dias=0 while True: linea = sys.stdin.readline() if not linea: break # print(linea) separado=linea.split(&#39;,&#39;) # 1 es numeroDia, 2 es nombreDia x = int(separado[1]) y = [int(i) for i in separado[3:27]] #print(separado) #print(y) todos[x].append(y) dias+=1 fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(9, 4)) axes[0][0].set_title(&#39;Lunes&#39;) axes[0][1].set_title(&#39;Martes&#39;) axes[0][2].set_title(&#39;Miercoles&#39;) axes[0][3].set_title(&#39;Jueves&#39;) axes[1][0].set_title(&#39;Viernes&#39;) axes[1][1].set_title(&#39;Sabado&#39;) axes[1][2].set_title(&#39;Domingo&#39;) numdia = 0 for j in range(2): for i in range(4): if not (j == 1 and i &gt; 2): dia = np.array(todos[numdia]) numdia+=1 bar_l = [i+1 for i in range(24)] performance=dia.mean(0) error=dia.std(0) # axes[j][i].barh(bar_l, performance, xerr=error, align=&#39;center&#39;,alpha = 0.5, color=&#39;green&#39;, ecolor=&#39;gray&#39;) axes[j][i].errorbar(bar_l, performance, yerr=error, fmt=&#39;o&#39;) # adding horizontal grid lines #for ax in axes: # ax.yaxis.grid(True) # ax.set_xticks([y+1 for y in range(len(all_data))]) # ax.set_xlabel(&#39;xlabel&#39;) # ax.set_ylabel(&#39;ylabel&#39;) print(&quot;dias: {}&quot;.format(dias)) # add x-tick labels #plt.setp(axes, xticks=[y+1 for y in range(len(all_data))], # xticklabels=[&#39;x1&#39;, &#39;x2&#39;, &#39;x3&#39;, &#39;x4&#39;]) plt.show() search-frequency from datetime import timedelta, datetime import json import sys import operator def daterange(start_date, end_date): for n in range(int ((end_date - start_date).days)): yield start_date + timedelta(n) if __name__ == &quot;__main__&quot;: while True: x = sys.stdin.readline() x = x.replace(&#39;\\n&#39;, &#39;&#39;) if not x: break # print(x) # mostrar nombre del archivo datemin=datetime.now() datemax=datetime.fromtimestamp(0/1e6) with open(x) as data_file: data = json.load(data_file) dias = {} i=0 for query in data[&#39;event&#39;]: query_text = query[&#39;query&#39;][&#39;query_text&#39;] timestamp = int(query[&#39;query&#39;][&#39;id&#39;][0][&#39;timestamp_usec&#39;]) date = datetime.fromtimestamp(timestamp/1e6) nombredia = date.strftime(&quot;%A&quot;) diasemana = date.weekday() if date &gt; datemax: datemax=date if date &lt; datemin: datemin=date hash = date.year * 10000 + date.month * 100 + date.day if hash in dias.keys(): dias[hash][date.hour+2]+=1 else: dias[hash]=[0 for i in range(24)] dias[hash].insert(0,nombredia) dias[hash].insert(0,diasemana) dias[hash][date.hour+2]+=1 # print(&quot;num dias con consultas: {}&quot;.format(len(dias))) for date in daterange(datemin, datemax): hash = date.year * 10000 + date.month * 100 + date.day if not hash in dias.keys(): nombredia = date.strftime(&quot;%A&quot;) diasemana = date.weekday() dias[hash]=[0 for i in range(24)] dias[hash].insert(0,nombredia) dias[hash].insert(0,diasemana) #print(&quot;faltaba: {}&quot;.format(hash)) #print single_date.strftime(&quot;%Y-%m-%d&quot;) sorted_x = sorted(dias.items(), key=operator.itemgetter(0)) for k, v in enumerate(sorted_x): width = len(v[1]) for j in range(width): if j == 0: print(&#39;{},&#39;.format(v[0]), end=&#39;&#39;) if j == width-1: print(&#39;{}&#39;.format(v[1][j])) else: print(&#39;{},&#39;.format(v[1][j]), end=&#39;&#39;) "],
["building-a-recommender-system.html", "Chapter 5 Building a Recommender System 5.1 Modular Application Framework 5.2 The User Interface 5.3 The Cloud and Back-end 5.4 Further Than Beta Version", " Chapter 5 Building a Recommender System It is well known that developing, managing and improving a recommender system are not trivial tasks, and one of the reasons is because sometimes researchers are not able to assess the value and usefulness of any specific insights or even a dataset simply by examining the data themselves, special care is needed to ensure that methods and procedures are properly documented, or this information is available and accessible.[6] Replication is one of the main matter of this project’s purpose, another goal is the improvement of models, even though evaluating Recommender Systems requires a definition of what constitutes a good system, and how this should be measured, and this is pretty complicated, maybe in further versions we will go deeper on this topic. Although is not going to be deprecated, that is why on this beta version, the user will help and contribute to improving with comments and bugs reports. If the data results in valuable insights, (or at least interesting), then they must not only be of good quality, they must be seen to be so. It is essential that methods are collected and compiled in line with agreed standards so that users are able to trust the data and are able to determine to the satisfaction that they are fit for their own purpose. Actually, the value of data is greatly increased if it can be compared with information from other sources, in this case, user’s own knowledge, it is expected that they found the insights logical, in order to generate engagement.[7] So as it is been said so, is important to consider accuracy and satisfaction to get engagement, in fact, accuracy is the first factor that contributes to a good recommender, i.e. its capacity to satisfy the individual users information needs.[8]. In other words, how well a recommender system performs this task is reflected by its accuracy: the more relevant, and the less irrelevant items it recommends, the more accurate it is, items that satisfy the information needs are “relevant” to the user. Following this path, the second factor that contributes to a good recommender system is its ability to provide “satisfaction” to the user. At first glance, one may assume that an accurate recommender system, however, many additional factors influence user satisfaction. 5.1 Modular Application Framework Go Google Yourself! Is a web application created to be used for any kind of user who has a Google account, this application is built with a R-Shiny interface and a backend mounted in Python, everything works in a AWS cluster communicated and packaged in a Docker Swarm working with AWS S3 buckets. user-interface Basically the Data Product it is based on 3 sources of information: Mails, Locations and Searches. The user interface has a Browse button where the user can upload a Takeoutfile, once the data is put on S3 buckets a Data Pipeline starts, first with a Pre-processing step, then an unsupervised density-based Machine Learning algorithm, is used to find clusters on a map, these clusters will be the most crowded places of the person, presumably: home, school, work, etc. all those places are going to be filtered by the important topics found it on Searches and Mails using Latent Dirichlet Algorithm. It is important to say that handling these important locations is the basis for showing (recommending) in the application the places of interest to, around those points with the help of the Google Maps API, the application has the option of being able to decide the time window in which it is desired to visualize. On the other hand, if a user has a Data Science approach is free to replicate the app locally. In a Github repository a researcher can expect a functional application with setup instructions. The user should be able to clone a repository and run a docker service. It is mandatory download your own Google Takeout files, then the user interface will give instructions for uploading the files, the user visualizes which are their more crowded places through the time and its recommendations. In order to make a short summary, what the application does is: load the files, read the files and upload to cloud, create location clusters, after that locate the most important ones, then filter sites of interest with keywords, finally, shows the places of interest to recommend, when the user log out the application destroys the user files. 5.2 The User Interface 5.2.1 R-Shiny, Keep it Simple (Alfy) For the development of the application it was used Shiny as front-end, and the only prupose is visualize the output of the models and the descriptive data, to understand this, Shiny will only cares about how to get summary dataframes. Shiny is an open source R package that provides an elegant and powerful web framework for building web applications using R. Shiny helps out to turn the analyses into interactive web applications without requiring HTML, CSS, or JavaScript requirements.[9] For the sake of simplicity it is been chosed Flask as Back-end framework, to make everything work Flask is called a micro framework because it does not require particular tools or libraries. It has no database abstraction layer for itself, form validation, or any other components. However, Flask supports extensions that can add application features as if they were implemented here is used to run the cluster and the database.[10] 5.2.2 Google Maps API (Alaín) What is Google Maps API How google maps api works? How Clustering Models works? How the recommendations looks like? 5.3 The Cloud and Back-end 5.3.1 Amazon Web Services Our back-end implementation made use of two powerfull Amazon Web Services (AWS) tools, Amazon Simple Storage Service (S3) and Amazon Elastic Compute Cloud (EC2). The first, Amazon S3 known as storage for Internet, provides different interfaces REST, SOAP and BitTorrent that allow to store and obtain any amount of data, from anywhere on the web. Although S3 does not provide details on its design, clearly manages its data through an object storage arquitecture. This objects are organized into buckets. A bucket is a container used to store key/value pairs in S3 and can hold an unlimited amount of data so the user could potentially had just one bucket for all of his information. According to Amazon, this design aims at to provide scalability, high avalilability and low latency at commodity costs. [11] Amazon S3, also has implementations on different programming languages and third party connectors like Java, .NET, Python, PHP, Ruby, among others, that help on data migration to the cloud. Requests of resources are authorized using an Access Control List (ACL) associated with each bucket and object. The names and keys of the bucket must be unique and chosen so that the objects are addressable by HTTP URLs: [12] http://s3.amazonaws.com/bucket/key http://bucket.s3.amazonaws.com/key http://bucket/key (where bucket is a DNS CNAME record pointing to bucket.s3.amazonaws.com) Amazon EC2, is a Infraestructure as a Service (IaaS), which provides the user a capability to rent by hour (hence the term “elastic”) “instances” (i.e. virtual computers, which support different operative systems like Linux, FreeBSD and Microsoft Windows) on the cloud in a fast way, secure and with size or harware modifiable. Also, provides control of the geographical localtion of instances which allows optimization of the latency and the high redundancy level. [13] 5.3.2 Merge the Pieces The AWS tools mentioned above were combined with Luigi and Docker, with the aim of creating a modular, with high computing power, replicable and scalable architecture. The first objective was achieved using Luigi, which is a Python module that helped us building complex pipelines of batch jobs. It handles dependency resolution, workflow management, visualization through a web interface, among other features. It also comes with Hadoop support built in. A pipeline is a set of data processing elements or tasks, in Luigi this tasks form a Directed Aciclic Graph (DAG), where the output of each task is the input of the next one, our DAG is displayed below. [14] Overview of the pipeline executed on Luigi A Luigi task has the following structure: import luigi class MyTask(luigi.Task): param =luigi.Parameter() def requires(self): return SomeOtherTask(self.param) def run(self): f = self.output.open(&#39;w&#39;) print &gt;&gt;f, &quot;hello world!&quot; f.close() def output(self): return luigi.LocalTarget(&#39;/tmp/foo/bar-%s&#39;.format(self.param)) if __name__ == &#39;__main__&#39;: luigi.run() This tasks can execute obviously Python code, but also bash commands and with this the ability to run any programming language indirectly or even create a docker container inside a task, which gather all the requirements needed to run that task. Tasks are often executed in parallel or in time-sliced fashion. In our tasks we share data between them using Amazon S3, where we also update indicators that the front-end reads to update the pipeline overall state to display it to the user. A template to create a Luigi pipeline using Docker is available at https://github.com/nanounanue/pipeline-template. Docker was a key piece of our arquitecture because of its incredible replication solution and its philosopy related to the old Java promise of write once, run anywhere. In a technical definition, Docker is an open source engine that quickly wraps up any application and all its peculiar dependencies in a lightweight, portable, self-sufficient container that can run virtually anywhere on anybody’s infrastructure. [15] Docker uses the resource isolation features of the Linux kernel such as cgroups and kernel namespaces, and a union-capable file system such as OverlayFS and others [16] to allow independent “containers” to run within a single Linux instance, avoiding the overhead of starting and maintaining virtual machines. [17] Taking this in consideration the last two objectives of our arquitectures were achieved using the tools Docker Engine and Docker Swarm. The latter, like its name states helped us build a swarm, this is a cluster of Docker engines, or nodes (related to a isolated virtual machine), where one can deploy services. The Docker Engine CLI and API include commands to manage swarm nodes (e.g., add or remove nodes), and deploy and orchestrate services across the swarm. [18] Each one of this nodes was an Amazon EC2 instance and on top of this nodes we setup a Docker swarm and deploy a Luigi server as a service, the latter was able to run as many Luigi workers as required. The swarm manager used its internal load balancing to distribute the Luigi tasks among services within the cluster, achieving with this high replication and scalability. The complete overview of the arquitecture can be observed on the image below, starting on the left side, the Amazon EC2 instances act as a base of our infraestructure on top of this instances different nodes support a Docker swarm, which is running a Luigi server service in charge of running Docker containers. This containers execute each one of the pipeline tasks related to the processing of mail, searches or locations. back-end Amazon S3 do the storage part of every tasks, and also using indicators (objects with 0/1 values) report to the front-end the status about of each one of the pipeline tasks. 5.4 Further Than Beta Version Integrate more google data applications Improve ML algorithms Formal testing for user expirience "],
["conclusion.html", "Chapter 6 Conclusion", " Chapter 6 Conclusion It is clear to see that the definition and construction of a Data Pipeline for an application is the product of a correct and deep analysis, not only because a data product has to follow all the formal phases, but also combine the quality approaches when is building a web app, the user experience is now part of every technological development. This is the path Data Scientists have to keep following and to which have been added proposers and that will set the course of a global trend, one might say many of these proposals may inspire people to apply the abstraction of these models on problems of other fields of knowledge. Creating a data product is a challenge that few assume but can definitely contribute a lot when it is created with meaning and aim to solve some problem that is left unattended. Once the construction process is started, something that must be taken into consideration is working in an isolated environment in order to guarantee the replicability of the product. Today there are different tools that help a project to be driven as a continuous and multilateral contributed. Finally, it is important to emphasize that combining Open Science approach with useful Web applications using Data Science, merge to create a very powerful complete weapon that has the ability to attack a wide variety of problems as reviewed in The present document, that is why we must continue to promote this type of studies and continue to ask questions in various fields of science. "],
["bibliografy.html", "Chapter 7 Bibliografy", " Chapter 7 Bibliografy [1] Fitzpatrick, Brian. “The Data Liberation Front Delivers Google Takeout” (June 28, 2011) Retrieved from http://dataliberation.blogspot.mx/2011/06/data-liberation-front-delivers-google.html [2] ico.org.uk “Determining what is personal data” (Dec 12, 2012) Pages 6-12 [3] Francesco Ricci, Lior R. and Bracha S. “Introduction to Recommender Systems Handbook” (2011) Springer Science+Business Media [4] Hosein Jafarkarimi; A.T.H. Sim and R. Saadatdoost “A Naïve Recommendation Model for Large Databases” (June 2012), International Journal of Information and Education Technology [5] itl.nist.gov “What is EDA?” (June 27, 2012) National Institute of Standarts and Technology U.S. [6] Graham Eele. “Building Statistical Capacity” (July 2015) Partnership in Statistics for Development in the 21st Century. Discussion Paper No. 7 [7] Beel J., Langer S. et al. “Research Paper Recommender System Evaluation: A Quantitative Literature Survey” (2015) Pages 15-24 [8] J.L. Herlocker, J.A. Konstan, L.G. Terveen, and J.T. Riedl, “Evaluating collaborative filtering recommender systems” ACM Transactions on Information Systems (TOIS) [9] Retrieved from http://rstudio.github.io/shiny/tutorial/#welcome on May, 2017 [10] Retrieved from http://flask.pocoo.org/docs/0.10/foreword/#what-does-micro-mean on May, 2017 [11] Amazon S3, Cloud Computing Storage for Files, Images, Videos. aws.amazon.com (2017). Retrieved May 22, 2017, from https://aws.amazon.com/es/s3/ [12] Amazon S3. (2017, April 12). In Wikipedia, The Free Encyclopedia. Retrieved May 22, 2017, from https://en.wikipedia.org/w/index.php?title=Amazon_S3&amp;oldid=775022059 [13] LaMonica, Martin (March 27, 2008). “Amazon Web Services adds ‘resiliency’ to EC2 compute service”. CNet News. Retrieved May 22, 2017, from https://www.cnet.com/news/amazon-web-services-adds-resiliency-to-ec2-compute-service/ [14] Erik Bernhardsson et al, “Luigi”. Github. Retrieved May 22, 2017, from https://github.com/spotify/luigi [15] O’Gara, Maureen (26 July 2013). “Ben Golub, Who Sold Gluster to Red Hat, Now Running dotCloud”. SYS-CON Media. Retrieved May 22, 2017, from http://maureenogara.sys-con.com/node/2747331. [16] “Select a storage driver documentation”. Docker documentation. Archived from the original on 2016-12-06. Retrieved May 22, 2017, from https://docs.docker.com/engine/userguide/storagedriver/selectadriver/. [17] “Docker Documentation: Kernel Requirements”. docker.readthedocs.org. 2014-01-04. Archived from the original on 2014-08-21. Retrieved May 22, 2017, from https://web.archive.org/web/20140821065734/http://docker.readthedocs.org/en/v0.7.3/installation/kernel/. [18] “Swarm mode key concepts”. docs.docker.com. Retrieved May 22, 2017, from https://docs.docker.com/engine/swarm/key-concepts/. "]
]
